{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下仅为本人第一次 LoRA 微调所用程序，仍然需要进一步的修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setting GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "# Parameter-Efficient-Fine-Tuning\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setting the finetuning parameters and some constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model\n",
    "MODEL_PATH = \"../models/chat/llama-3-chinese-8b-instruct-v2\"\n",
    "# Size of each batch for a single gradient update\n",
    "MICRO_BATCH_SIZE = 4\n",
    "# Total size of the batch\n",
    "BATCH_SIZE = 128\n",
    "# Number of steps to accumulate gradients before performing a backward/update pass\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "# Number of training epoch\n",
    "EPOCHS = 3\n",
    "# Learning rate for the optimizer\n",
    "LEARNING_RATE = 3e-4\n",
    "# Maximum sequence length for tokenized input\n",
    "CUTOFF_LEN = 256\n",
    "# Hyperparameters specific to LoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "# Size of the validation set \n",
    "VAL_SET_SIZE = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the model into the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting to load the model into memory\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    use_fast=False, \n",
    "    add_eos_token=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Successfully loaded the model into memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setting LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-3-8b 模型结构\n",
    "(model): LlamaModel(\n",
    "  (embed_tokens): Embedding (128256, 4096)\n",
    "  (layers): ModuleList(\n",
    "    (0-31): 32 x LlamaDecoderLayer ( \n",
    "      (self._attn): LlamaSdpaAttention(\n",
    "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "        (k_proj): Linear(in_features=4096, out _features=1024, bias=False) \n",
    "        (v_proj): Linear (in_features=4096, out_features=1024, _features=1024, bias=False)\n",
    "        (o_proj): Linear(in_reatures=4096, out out_features=4096, bias=False)\n",
    "        (rotary_emb): LlamaRotaryEmbedding ()\n",
    "      )\n",
    "      (mlp) : LlamaMLP(\n",
    "        (gate_proj ): Linear (in_reatures=4096, out_reatures=14336, bias=false)\n",
    "        (up_proj): Linear(in_reatures=4096, out_reatures=14336, bias=False) \n",
    "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "        (act_fn) : SiLU()\n",
    "      )\n",
    "      (input_layernorm): LlamaRMSNorm ( )\n",
    "      (post_attention_layernorm) : LlamaRMSNorm()\n",
    "    )\n",
    "  )\n",
    "  (norm): LlamaRMSNorm ( )\n",
    ")\n",
    "(Im_head): Linear (in_features=4096, out_features=128256, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r(rank):\n",
    "• 定义: 低秩矩阵 (A) 和 (B) 的秩。它决定了新的权重矩阵在低秩空间中的维度。\n",
    "• 作用: 秩 (r) 越大，模型的表达能力越强，但需要调整的参数也越多。较小的 (r) 值能够减少参数数量，从而降低计算成本，但可能会损失部分模型性能。\n",
    "• 设置建议: 一般选择 1 到 64 之间的值。较小的模型或较小的数据集可以选择较小的 (r) 值，而较大的模型和数据集可以选择较大的 (r) 值。\n",
    "# lora_alpha:\n",
    "• 定义: 用于缩放低秩矩阵 (A) 和 (B) 的比例因子。\n",
    "• 作用: 控制低秩矩阵在权重更新中的影响。较大的 lora_alpha 值会增加低秩矩阵对模型权重的影响，反之则减小。\n",
    "• 设置建议: 通常设置为与 r 相同或较接近的值。比如，如果 r 为 16，那么 lora_alpha 可以设为 16。\n",
    "# lora_dropout:\n",
    "• 定义: 在应用低秩矩阵之前对其进行 Dropout 操作的概率。\n",
    "• 作用: Dropout 用于正则化，防止模型过拟合。lora_dropout 控制在应用低秩矩阵前丢弃一些元素的概率。\n",
    "• 设置建议: 一般选择 0 到 0.5 之间的值。可以根据训练过程中模型的表现调整这个值。如果模型过拟合，可以增加 lora_dropout；如果模型欠拟合，可以减少或设置为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0  \n",
    "data = load_dataset(\"json\", data_files=\"../data/alpaca_data.json\")\n",
    "\n",
    "# Split the original alpaca dataset to training dataset and validation dataset\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why setting pad_token_id to 0?\n",
    "• By setting the pad_token_id to 0, which typically represents an unknown token (often <unk> in many tokenizers), you ensure that padding tokens are easily distinguishable from other tokens in the sequence. \n",
    "• This helps the model to ignore padding tokens during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a prompt in a certain format.\n",
    "def generate_prompt(data_point):\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "\n",
    "                ### Input:\n",
    "                {data_point[\"input\"]}\n",
    "\n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "\n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Narrow the length of input_id to max_length.\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"][:-1],\n",
    "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why trim the last token?\n",
    "• Sequence Length Management:\n",
    "    By setting max_length=CUTOFF_LEN + 1, the function ensures that even after trimming, the resulting sequence length is CUTOFF_LEN. This is useful for controlling the sequence length precisely.\n",
    "• Consistency with Model Input Requirements:\n",
    "    Some models might require sequences to be of a specific length or have certain constraints on input sizes. Trimming ensures that the tokenized sequences meet these requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)))\n",
    "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffle() method\n",
    "• This method shuffles the dataset to randomize the order of the data points. Shuffling is important to prevent the model from learning any spurious patterns due to the order of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lambda x: tokenize(generate_prompt(x))\n",
    "lambda is a way to definite a function in Python.\n",
    "# Which equals:\n",
    "def function(x):\n",
    "    return tokenize(generate_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        # Number of steps between logging training metrics.\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        # Number of steps between evaluations.\n",
    "        eval_steps=200,\n",
    "        # Number of steps between saving checkpoints.\n",
    "        save_steps=200,\n",
    "        output_dir=\"lora_weight/llama-3\",\n",
    "        # The maximum number of checkpoints to keep. Older checkpoints will be deleted to save space.\n",
    "        save_total_limit=3,\n",
    "        # Whether to load the best model found during training at the end of the training process.\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    # Prepare batches of data.\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disables caching in the model’s configuration. (Save memory)\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "# The custom implementation uses get_peft_model_state_dict to modify the behavior of the original state_dict method.\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lora weight\n",
    "lora_path = './llama3_lora'\n",
    "trainer.model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
